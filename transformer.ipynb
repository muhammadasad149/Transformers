{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformers**\n",
    "\n",
    "A transformer is a neural network architecture used for sequence-to-sequence data tasks such as text summarization, question answering, and machine translation. In the diagram of the transformer (Attention is All You Need), there are two blocks: one for the encoder and the other for the decoder. Both blocks use self-attention instead of LSTM. Self-attention aids in parallel processing, which accelerates data training and scalability.\n",
    "\n",
    "![Transformer _model_architecture](Transformer_model_architecture..png)\n",
    "\n",
    "**History of Transformers**\n",
    "\n",
    "The transformer is built on the \"Attention is All You Need\" research paper introduced by Google Brain in 2017. Initially developed for machine translation, it is now utilized for various purposes.\n",
    "\n",
    "**Impact of Transformers in NLP**\n",
    "\n",
    "- **Revolution in NLP:** For about 50 years, humans have worked on NLP using heuristic, statistical models, embedding models, N-gram, bag-of-words, RNNs, GRU, LSTM, etc. Transformers, however, have provided state-of-the-art results, with GPT being a prominent example.\n",
    "\n",
    "- **Democratizing AI:** Before transformers, building NLP applications from scratch was common. Now, fine-tuning large models like BERT and GPT for specific domains yields state-of-the-art results.\n",
    "\n",
    "- **Multimodal Capability:** Transformers aren't limited to NLP; they are used in various domains such as speech-to-text, text-to-image, and text-to-video.\n",
    "\n",
    "- **Acceleration of GenAI:** Transformers have revolutionized image generation and other Generative AI domains like GANs.\n",
    "\n",
    "- **Unification of Deep Learning:** Transformers are used in NLP, GenAI, computer vision, reinforcement learning, and more.\n",
    "\n",
    "**Why Transformers Were Created?**\n",
    "\n",
    "Sequence-to-sequence learning with neural networks involves two blocks: encoder and decoder. Previously, LSTM was commonly used, but it had limitations with large context vectors. The introduction of attention mechanisms in neural machine translation solved this issue. However, sequential training in these models was slow.\n",
    "\n",
    "**Attention is All You Need**\n",
    "\n",
    "This research paper introduced the transformer, which replaced LSTM with self-attention. This led to parallel processing, smaller components, stability, and robustness, along with the use of different hyperparameters.\n",
    "\n",
    "**Timeline of Transformers**\n",
    "\n",
    "- **2000-2014:** RNNs/LSTMs - The Origin Story\n",
    "- **2014:** Attention\n",
    "- **2017:** Transformer\n",
    "- **2018:** BERT/GPT (Transfer Learning)\n",
    "- **2018-2020:** Vision Transformer/Alphasord. 2\n",
    "- **2021:** Gen AI\n",
    "- **2022:** CharGPT/Stable Diffusion\n",
    "\n",
    "**Advantages of Transformers**\n",
    "\n",
    "- Scalability\n",
    "- Transfer learning\n",
    "- Multimodal capability\n",
    "- Flexible architecture (e.g., BERT for encoder, GPT for decoder)\n",
    "- Ecosystem support (e.g., Hugging Face)\n",
    "- Integrated AI technologies (e.g., GANs + Transformers, RL + Transformers, CV + Transformers)\n",
    "\n",
    "**Disadvantages of Transformers**\n",
    "\n",
    "- High computational cost\n",
    "- Requires a huge amount of data\n",
    "- Overfitting\n",
    "- Energy consumption\n",
    "- Interpretability issues\n",
    "- Bias (ethical concerns with data)\n",
    "\n",
    "**Self Attention**\n",
    "\n",
    "Self-attention is a mechanism that converts static word embeddings into contextual embeddings, suitable for various NLP applications.\n",
    "\n",
    "**Self Attention in Transformers**\n",
    "\n",
    "- Various types of encoding: hot encoding, bag of words, embedding, word embedding, contextual embedding.\n",
    "- Word embedding is used to convert sentences into semantic vectors.\n",
    "- Self-attention converts word embeddings into contextual embeddings dynamically.\n",
    "- Calculated self-attention: For instance, given the sentence \"money bank grows,\" each word is converted into a new embedding based on self-attention calculations. the calculation is given down in the iamge\n",
    "![self attention machanism](self_attention.png)\n",
    "\n",
    "- **point of consideration:**\n",
    "\n",
    "1- this operation is in parallel but the disadvantage is that the which word is used with which words \n",
    "\n",
    "![parallel operation](parallel_operation.png)\n",
    "\n",
    "2- there are perameter involved its mean their is no weight and bais involved therefore this emddeing is call general contextual embedding(mean that that the new embedding is generated is general is not spacific menn taht its is not learnable becaue in it yhey dont use the parameters)\n",
    "\n",
    "![no learning](no_learning.png)\n",
    "\n",
    "**progress**\n",
    "\n",
    "\n",
    "![progress](progress.png)\n",
    "\n",
    "\n",
    "in the progress digram we see we that how to feed the word to the machine for their we introduce the the word embedding now the disadvantage is that this embedding does't no the context of the words than we convert the word embedding into the contextual embedding which is also khow as general context embedding the drawback of the general context embeding is that it is not learnable parameters for learn the parameters we use the task specific embedding \n",
    "\n",
    "- to overcome the learnable parameters it introduce to convert the word embedding into the three new embedding query value and key \n",
    "\n",
    "![three new embedding](three_new_embedding.png)\n",
    "\n",
    "- from one vector to born new vector we use linear transformation(is the process where we multiple the random weights matrix which is the data on which the model is traing whith the word embeding) from result we get wq,wk,wv this process is perform untill we got best weights at the end we got new three matrix query,key,and value\n",
    "\n",
    "![formation](formation.png)\n",
    "\n",
    "- now we use this three new matrix in self attention calulation at the end we got task specific embedding \n",
    "\n",
    "\n",
    "![calulation](calulation.png)\n",
    "\n",
    "- all above process we do in parrelle we have 3 word or 3000k work we do this in parallel at show in the image\n",
    "\n",
    "![self attention parallel](self_attention_parallel.png)\n",
    "\n",
    "- The overall summary of the discussion above can be encapsulated in the formula: Attention(Q, K, V) = softmax(QKᵀ/√dk)V. However, in the actual formula, Attention(Q, K, V) = softmax(QKᵀ/√dk)V, we observe that QKᵀ is divided by √dk. Here, 'dk' represents the dimension length. It is divided by √dk because low dimensions have low variance, and high dimensions have high variance. Thus, dividing QKᵀ by the square root of dk ensures normalization. Additionally, dividing by √dk helps to mitigate the problem of vanishing gradients.This process is called Scaled Dot Product Attention\n",
    "\n",
    "![detail](detail.png)\n",
    "\n",
    "### TRANSFORMER ACRHITECHTURE\n",
    "\n",
    "![Transformer _model_architecture](Transformer_model_architecture..png)\n",
    "\n",
    "Detailed Notes:\n",
    "Summary of Transformer Networks\n",
    "\n",
    "Transformer networks are deep learning models designed to mimic the human process of attention. They excel in tasks involving sequence understanding and generation, such as natural language processing and image recognition.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "Attention: Transformers attend to elements of a sequence, capturing dependencies and relationships.\n",
    "Encoder: The encoder processes the input sequence, generating a representation that captures its context and meaning.\n",
    "Decoder: The decoder generates the output sequence, one element at a time, using information from the encoder and its own attention mechanisms.\n",
    "Encoder Architecture:\n",
    "\n",
    "Consists of multiple encoder layers, each with:\n",
    "Multi-Head Self-Attention (MHSA): Captures relationships between elements in the sequence.\n",
    "Feed-Forward Neural Network (FFNN): Processes information from MHSA.\n",
    "Adds Positional Encoding to input embeddings to convey element order.\n",
    "Decoder Architecture:\n",
    "\n",
    "Consists of multiple decoder layers, each with:\n",
    "Masked Self-Attention (MSA): Ensures elements in the output sequence only attend to previous elements.\n",
    "Multi-Head Attention (MHA): Attends to encoder output to gather relevant information.\n",
    "FFNN: Further processes information from MHA.\n",
    "How Transformers Work:\n",
    "\n",
    "Input sequence is converted to embeddings and positional encodings.\n",
    "Encoders process the embeddings, capturing contextual information and relationships.\n",
    "Decoder receives encoder output and attends to both the output and its own sequence.\n",
    "Decoder generates the output sequence, one element at a time, using information from attention and feed-forward layers.\n",
    "Transformer model adjusts weights throughout training to optimize output accuracy.\n",
    "Benefits:\n",
    "\n",
    "Can handle long-range dependencies effectively.\n",
    "Efficient processing by processing all elements in parallel.\n",
    "Improved performance in tasks involving sequence understanding and generation.\n",
    "Applications:\n",
    "\n",
    "Natural language processing: Translation, summarization, language modeling\n",
    "Image recognition and understanding\n",
    "Sequence-to-sequence modeling: Time series forecasting, speech recognition\n",
    "\n",
    "\n",
    "### Vision Transformers\n",
    "\n",
    "![Vision Transformers](vt1.png)\n",
    "\n",
    "![Vision Transformers](vt2.png)\n",
    "\n",
    "![Vision Transformers](vt3.png)\n",
    "\n",
    "![Vision Transformers](vt4.png)\n",
    "\n",
    "![Vision Transformers](vt5.png)\n",
    "\n",
    "![Vision Transformers](vt6.png)\n",
    "\n",
    "\n",
    "## Summary of Vision Transformers \n",
    "\n",
    "Introduction:\n",
    "\n",
    "Vision Transformers (ViTs) are deep learning models designed for computer vision tasks, inspired by the success of Transformers in natural language processing. ViTs employ a self-attention mechanism to understand relationships between different parts of an image.\n",
    "\n",
    "Image Segmentation and Tokenization:\n",
    "\n",
    "ViTs divide an image into smaller patches, which are then flattened into sequences of tokens. This process enables the Transformer model to process and understand image content more efficiently.\n",
    "\n",
    "Architecture of Vision Transformers:\n",
    "\n",
    "- Encoder: A stack of layers, including self-attention layers, feed-forward networks, and a classification layer. The encoder extracts meaningful features and understands spatial relationships within the image.\n",
    "- No Decoder: Unlike traditional Transformers, ViTs lack a decoder because the primary goal is to understand the image content rather than generate output sequences.\n",
    "\n",
    "Benefits of Vision Transformers:\n",
    "\n",
    "- Ability to understand global and local relationships within an image.\n",
    "- Reduced dimensionality of input vectors, improving computational efficiency and noise reduction.\n",
    "- Extraction of essential features and elimination of irrelevant variations.\n",
    "\n",
    "Key Differences from Traditional Transformers:\n",
    "\n",
    "- Absence of a decoder.\n",
    "- Self-attention layers tailored for computer vision tasks.\n",
    "- Focus on understanding image content rather than generating output sequences.\n",
    "\n",
    "Applications of Vision Transformers:\n",
    "\n",
    "- Image classification\n",
    "- Object detection\n",
    "- Other computer vision tasks\n",
    "\n",
    "\n",
    "### SWIN TRANSFORMER\n",
    "\n",
    "![SWIN TRANSFORMER](swin_t.png)\n",
    "\n",
    "Summary:\n",
    "\n",
    "The Swin Transformer (SWin-T) architecture, introduced in 2021, makes advancements in Vision Transformers by effectively handling high-resolution images with lower computational complexity.\n",
    "\n",
    "Architecture:\n",
    "\n",
    "SWin-T consists of:\n",
    "\n",
    "- Patenization: Dividing the input image into patches.\n",
    "- Linear Embedding: Converting image pixels into vectors for Transformer layers.\n",
    "\n",
    "SWin-T Blocks:\n",
    "- Each block has two subunits with normalization layers, attention modules, and multi-layer perceptron layers.\n",
    "- The first subunit uses window multi-head self-attention while the second uses shifted window multi-head self-attention.\n",
    "\n",
    "Window-based Multi-Head Self-Attention:\n",
    "- Computes relationships between patches within local windows.\n",
    "- Shifted Window Multi-Head Self-Attention:\n",
    "- Maintains connections between windows while preserving computational efficiency by shifting windows and performing cyclic shifts on patches.\n",
    "\n",
    "Patch Merging:\n",
    "- Selectively merges adjacent patches to capture global information.\n",
    "- Reduces image resolution by a factor of 2 at each merging stage.\n",
    "\n",
    "Task-Specific Head:\n",
    "- For image classification, a linear layer is applied to the final output.\n",
    "- For object detection or segmentation, outputs from all stages are provided to task-specific heads.\n",
    "\n",
    "Variants:\n",
    "\n",
    "There are four variants of SWin-T: Tiny, Small, Base, and Large, each differing in parameters and number of layers.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Effectively handles high-resolution images.\n",
    "Lower computational complexity than Vision Transformers.\n",
    "Provides features from different resolutions for object detection and segmentation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
